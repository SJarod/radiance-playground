
@article{alexander_radiance_nodate,
	title = {Radiance Cascades:  A Novel Approach to Calculating Global  Illumination[{WIP}]},
	url = {https://drive.google.com/file/d/1L6v1_7HY2X-LV3Ofb6oyTIxgEaP4LOI6},
	author = {Alexander, Sannikov},
}

@online{noauthor_real-time_nodate,
	title = {Real-Time Global Illumination using Precomputed Light Field Probes {\textbar} Research},
	url = {https://research.nvidia.com/publication/2017-02_real-time-global-illumination-using-precomputed-light-field-probes},
	urldate = {2025-01-17},
	file = {Real-Time Global Illumination using Precomputed Light Field Probes | Research:C\:\\Users\\Jarod\\Zotero\\storage\\5XARXUX8\\2017-02_real-time-global-illumination-using-precomputed-light-field-probes.html:text/html},
}

@online{team_radiance_2020,
	title = {Radiance vs. Irradiance: The Differences},
	url = {https://gamma-sci.com/2020/12/16/radiance-vs-irradiance-the-differences/},
	shorttitle = {Radiance vs. Irradiance},
	abstract = {The display measurement industry is incredibly complex. Experts in the field have to be laser-focused all the time, so they don’t make mistakes. If you’ve ever talked to someone in […]},
	titleaddon = {Gamma Scientific},
	author = {Team, Gamma Editorial},
	urldate = {2025-01-12},
	date = {2020-12-16},
	langid = {american},
	file = {Snapshot:C\:\\Users\\Jarod\\Zotero\\storage\\7YC8G6WD\\radiance-vs-irradiance-the-differences.html:text/html},
}

@online{noauthor_home_nodate,
	title = {Home},
	url = {//www.vkguide.dev/},
	abstract = {Guide to the Vulkan {API}},
	titleaddon = {Vulkan Guide},
	urldate = {2025-01-12},
	langid = {american},
	file = {Snapshot:C\:\\Users\\Jarod\\Zotero\\storage\\3BPWM7YX\\vkguide.dev.html:text/html},
}

@online{noauthor_introduction_nodate,
	title = {Introduction - Vulkan Tutorial},
	url = {https://vulkan-tutorial.com/},
	urldate = {2025-01-12},
	file = {Introduction - Vulkan Tutorial:C\:\\Users\\Jarod\\Zotero\\storage\\XEFH75CM\\vulkan-tutorial.com.html:text/html},
}

@software{kapoulkine_zeuxvolk_2025,
	title = {zeux/volk},
	rights = {{MIT}},
	url = {https://github.com/zeux/volk},
	abstract = {Meta loader for Vulkan {API}},
	author = {Kapoulkine, Arseny},
	urldate = {2025-01-12},
	date = {2025-01-11},
	note = {original-date: 2018-03-12T03:46:10Z},
	keywords = {vulkan},
}

@online{wwwsaschawillemsde_vulkan-tutorial_nodate,
	title = {vulkan-tutorial},
	url = {https://www.saschawillems.de/tags/vulkan-tutorial/},
	titleaddon = {Sascha Willems},
	author = {www.saschawillems.de},
	urldate = {2025-01-12},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Jarod\\Zotero\\storage\\XEVFU9I3\\vulkan-tutorial.html:text/html},
}

@online{noauthor_mgp2_nodate,
	title = {{MGP}2 2024-2025 Sujets des projets de Spécialisation},
	url = {https://docs.google.com/spreadsheets/d/1YqF0I9k2nvqEno0DW6XoWpPCXFIG_Pij2QykXkerYPw/edit?gid=0&usp=embed_facebook},
	titleaddon = {Google Docs},
	urldate = {2024-10-24},
	langid = {french},
	file = {Snapshot:C\:\\Users\\Jarod\\Zotero\\storage\\U2QIQTFV\\edit.html:text/html},
}

@video{simondev_exploring_2024,
	title = {Exploring a New Approach to Realistic Lighting: Radiance Cascades},
	url = {https://www.youtube.com/watch?v=3so7xdZHKxw},
	shorttitle = {Exploring a New Approach to Realistic Lighting},
	abstract = {Radiance Cascades are an innovative solution to global illumination from the devs of Path of Exile 2. Let's explore and implement their approach.

Gamedev Courses: https://simondev.io
Support me on Patreon:   / simondevyt  

Follow me on:
Instagram:   / beer\_and\_code  
Twitter:   / iced\_coffee\_dev  

In this video, we look through the recent Radiance Cascades paper from the devs of Path of Exile 2, explore the ideas behind their approach, and implement something similar.

Reference:
Paper: https://drive.google.com/file/d/1L6v1...
Discord:   / discord  

Other great explanations:
{GM} Shaders: https://mini.gmshaders.com/p/radiance...
Tmpvar: https://tmpvar.com/poc/radiance-casca...},
	author = {{SimonDev}},
	urldate = {2024-10-24},
	date = {2024-07-02},
}

@online{noauthor_exploring_nodate,
	title = {Exploring a New Approach to Realistic Lighting: Radiance Cascades - {YouTube}},
	url = {https://www.youtube.com/watch?v=3so7xdZHKxw},
	urldate = {2024-10-24},
	file = {Exploring a New Approach to Realistic Lighting\: Radiance Cascades - YouTube:C\:\\Users\\Jarod\\Zotero\\storage\\EE5S8XNL\\watch.html:text/html},
}

@online{noauthor_journal_nodate,
	title = {Journal of Computer Graphics Techniques},
	url = {https://jcgt.org/},
	urldate = {2024-10-24},
	file = {Journal of Computer Graphics Techniques:C\:\\Users\\Jarod\\Zotero\\storage\\GKYAJ2B4\\jcgt.org.html:text/html},
}

@online{noauthor_building_nodate,
	title = {Building Real-Time Global Illumination: Radiance Cascades},
	url = {https://jason.today/rc},
	shorttitle = {Building Real-Time Global Illumination},
	abstract = {An interactive walkthrough of implementing radiance cascades, a technique for real-time noiseless global illumination.},
	urldate = {2024-10-24},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Jarod\\Zotero\\storage\\MMUWDYRJ\\rc.html:text/html},
}

@online{noauthor_radiancecascadespdf_nodate,
	title = {{RadianceCascades}.pdf},
	url = {https://drive.google.com/file/u/1/d/1L6v1_7HY2X-LV3Ofb6oyTIxgEaP4LOI6/view?usp=embed_facebook},
	titleaddon = {Google Docs},
	urldate = {2024-10-24},
	file = {Snapshot:C\:\\Users\\Jarod\\Zotero\\storage\\BKHTBVGT\\view.html:text/html},
}

@online{noauthor_isart_digital_template-pfem2-etudiantdocx_nodate,
	title = {{ISART}\_Digital\_template-{PFEM}2-etudiant.docx},
	url = {https://docs.google.com/document/d/107i07Rc6XQO7_uJRJgSG68sUfdrRoEdw/edit?usp=embed_facebook},
	abstract = {Informations concernant l’étudiant  Nom : spécialité {MGP}2 :  • {GPU} 	• {IA}  Ce projet se fera-t-il en binôme ?  • Oui        • Non Si oui, Nom de l’autre étudiant :  Ce ...},
	titleaddon = {Google Docs},
	urldate = {2024-10-24},
	langid = {french},
}

@video{alexander_sannikov_2d_2022,
	title = {2d Full Global Illumination \#2 (working principle and perf tests)},
	url = {https://www.youtube.com/watch?v=o2kgW2TBpUo},
	abstract = {In this video I'm showcasing my sampling strategy that's based on cascades. Cascades that are farther away, contain more information about ray directions, but less information about ray origins -- because of this, each cascade actually takes up exactly the same amount of memory.

In other words, cascades encode information in light probes. Cascade n+1 contains 4x less light probes than cascade n, but each probe is 4x higher resolution.

Note that performance of the algorithm is constant and does not depend on how many light sources there are : each pixel is assumed to have its own luminance and opacity values, {GI} is calculated in 1920x1080 full res. Also no information is cached : each frame lighting is calculated entirely from scratch, it converges fast enough so that no denoising is needed.

{PS} this is not unity and not {UE}, this is my custom vulkan-based framegraph rendering engine, called {LegitEngine}. The engine itself is open source with a bunch of demos, but this demo is not in the repo (yet?): https://github.com/Raikiri/{LegitEngine}
The profiler {ImGui} component that you see in the lower left corner can be used as a standalone: https://github.com/Raikiri/{LegitProfiler} (you still need to provide it your own time stamps and you need to have a working {ImGui} setup to use it).},
	author = {{Alexander Sannikov}},
	urldate = {2024-10-24},
	date = {2022-08-28},
}

@online{noauthor_24_nodate,
	title = {(24) Maxime {ROUFFET} {\textbar} {LinkedIn}},
	url = {https://www.linkedin.com/in/maximerouffet/?locale=fr_FR},
	urldate = {2024-10-24},
}

@video{unrealab_basic_2022,
	title = {Basic 3D lighting concepts, Ray Tracing and Global Illumination},
	url = {https://www.youtube.com/watch?v=b_I9fVenRIo},
	abstract = {The basics of 3D lighting: direct light, indirect light, ray tracing, shadows, Global Illumination and Final Gather.},
	author = {{UnreaLab}},
	urldate = {2024-10-24},
	date = {2022-03-14},
}

@video{alexander_sannikov_real_2021,
	title = {Real time optical wave field propagation},
	url = {https://www.youtube.com/watch?v=tto9ObpMGRI},
	author = {{Alexander Sannikov}},
	urldate = {2024-10-24},
	date = {2021-08-14},
}

@video{alexander_sannikov_radiance_2023,
	title = {Radiance Cascades Rendered Directly},
	url = {https://www.youtube.com/watch?v=xkJ6i2N32Pc},
	abstract = {In this video we explore data stored in radiance cascades by observing it directly. This is equivalent to precalculating a scene, storing a cross-section of its radiance field and then rendering it from any viewpoint and any angle in O(1).},
	author = {{Alexander Sannikov}},
	urldate = {2024-10-24},
	date = {2023-08-19},
}

@online{noauthor_dynamic_nodate,
	title = {Dynamic Diffuse Global Illumination with Ray-Traced Irradiance Fields ({JCGT})},
	url = {https://jcgt.org/published/0008/02/01/},
	urldate = {2025-01-17},
}

@online{noauthor_dynamic_nodate-1,
	title = {Dynamic Diffuse Global Illumination},
	url = {https://morgan3d.github.io/articles/2019-04-01-ddgi/},
	urldate = {2025-01-17},
}

@article{kaplanyan_light_nodate,
	title = {Light Propagation Volumes in {CryEngine} 3},
	author = {Kaplanyan, Anton},
	langid = {english},
	file = {PDF:C\:\\Users\\Jarod\\Zotero\\storage\\3WPE6LHL\\Kaplanyan - Light Propagation Volumes in CryEngine 3.pdf:application/pdf},
}

@online{noauthor_httpsadvancesrealtimerenderingcoms2009light_propagation_volumespdf_nodate,
	title = {https://advances.realtimerendering.com/s2009/Light\_Propagation\_Volumes.pdf},
	url = {https://advances.realtimerendering.com/s2009/Light_Propagation_Volumes.pdf},
	urldate = {2025-01-17},
	file = {https\://advances.realtimerendering.com/s2009/Light_Propagation_Volumes.pdf:C\:\\Users\\Jarod\\Zotero\\storage\\8H9XNN23\\Light_Propagation_Volumes.pdf:application/pdf},
}

@inreference{noauthor_radiosity_2024,
	title = {Radiosity (computer graphics)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Radiosity_(computer_graphics)&oldid=1259287953},
	abstract = {In 3D computer graphics, radiosity is an application of the finite element method to solving the rendering equation for scenes with surfaces that reflect light diffusely.  Unlike rendering methods that use Monte Carlo algorithms (such as path tracing), which handle all types of light paths, typical radiosity only account for paths (represented by the code "{LD}*E") which leave a light source and are reflected diffusely some number of times (possibly zero) before hitting the eye. Radiosity is a global illumination algorithm in the sense that the illumination arriving on a surface comes not just directly from the light sources, but also from other surfaces reflecting light. Radiosity is viewpoint independent, which increases the calculations involved, but makes them useful for all viewpoints.
Radiosity methods were first developed in about 1950 in the engineering field of heat transfer. They were later refined specifically for the problem of rendering computer graphics in 1984–1985 by researchers at Cornell University  and Hiroshima University.
Notable commercial radiosity engines are Enlighten by Geomerics (used for games including Battlefield 3 and Need for Speed: The Run); 3ds Max; form•Z; {LightWave} 3D and the Electric Image Animation System.},
	booktitle = {Wikipedia},
	urldate = {2025-01-17},
	date = {2024-11-24},
	langid = {english},
	note = {Page Version {ID}: 1259287953},
}

@inproceedings{dachsbacher_reflective_2005,
	location = {Washington District of Columbia},
	title = {Reflective shadow maps},
	isbn = {978-1-59593-013-2},
	url = {https://dl.acm.org/doi/10.1145/1053427.1053460},
	doi = {10.1145/1053427.1053460},
	abstract = {In this paper we present ”reﬂective shadow maps”, an algorithm for interactive rendering of plausible indirect illumination. A reﬂective shadow map is an extension to a standard shadow map, where every pixel is considered as an indirect light source. The illumination due to these indirect lights is evaluated on-the-ﬂy using adaptive sampling in a fragment shader. By using screen-space interpolation of the indirect lighting, we achieve interactive rates, even for complex scenes. Since we mainly work in screen space, the additional effort is largely independent of scene complexity. The resulting indirect light is approximate, but leads to plausible results and is suited for dynamic scenes. We describe an implementation on current graphics hardware and show results achieved with our approach.},
	eventtitle = {I3D05: Symposium on Interactive 3D Graphics and Games 2005},
	pages = {203--231},
	booktitle = {Proceedings of the 2005 symposium on Interactive 3D graphics and games},
	publisher = {{ACM}},
	author = {Dachsbacher, Carsten and Stamminger, Marc},
	urldate = {2025-01-17},
	date = {2005-04-03},
	langid = {english},
	file = {PDF:C\:\\Users\\Jarod\\Zotero\\storage\\JVR9Z87B\\Dachsbacher and Stamminger - 2005 - Reflective shadow maps.pdf:application/pdf},
}

@inproceedings{shcherbakov_dynamic_2019,
	title = {Dynamic Radiosity},
	isbn = {978-80-86943-37-4},
	url = {http://wscg.zcu.cz/wscg2019/2019-papers/!!_CSRN-2801-10.pdf},
	doi = {10.24132/CSRN.2019.2901.1.10},
	abstract = {In this paper we propose a novel radiosity implementation which we called dynamic radiosity. By storing and updating local form-factor matrix of the patches closest to the observer we have solved 2 main problems of radiosity algorithm: (1) quadratic complexity of the algorithm and thus difﬁculty of applying it to a large-scale scenes, and (2) the possibility of changing geometry on the ﬂy (dynamic geometry).},
	eventtitle = {{WSCG}'2019 - 27. International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision'2019},
	booktitle = {Computer Science Research Notes},
	publisher = {Západočeská univerzita},
	author = {Shcherbakov, Alexandr and Frolov, Vladimir},
	urldate = {2025-01-17},
	date = {2019},
	langid = {english},
	file = {PDF:C\:\\Users\\Jarod\\Zotero\\storage\\MA5RRJAE\\Shcherbakov and Frolov - 2019 - Dynamic Radiosity.pdf:application/pdf},
}

@online{noauthor_httpwwwklaygeorgmaterial3_12girsmpdf_nodate,
	title = {http://www.klayge.org/material/3\_12/{GI}/rsm.pdf},
	url = {http://www.klayge.org/material/3_12/GI/rsm.pdf},
	urldate = {2025-01-17},
	file = {http\://www.klayge.org/material/3_12/GI/rsm.pdf:C\:\\Users\\Jarod\\Zotero\\storage\\PTQFXX64\\rsm.pdf:application/pdf},
}

@online{noauthor_httpswwwkeldyshrupagescgrapharticlesdep20publ201919-43pdf_nodate,
	title = {https://www.keldysh.ru/pages/cgraph/articles/dep20/publ2019/19-43.pdf},
	url = {https://www.keldysh.ru/pages/cgraph/articles/dep20/publ2019/19-43.pdf},
	urldate = {2025-01-17},
	file = {https\://www.keldysh.ru/pages/cgraph/articles/dep20/publ2019/19-43.pdf:C\:\\Users\\Jarod\\Zotero\\storage\\KPDNBU6K\\19-43.pdf:application/pdf},
}

@online{arts_siggraph_2021,
	title = {{SIGGRAPH} 21: Global Illumination Based on Surfels},
	url = {https://www.ea.com/seed/news/siggraph21-global-illumination-surfels},
	shorttitle = {{SIGGRAPH} 21},
	abstract = {The {SIGGRAPH} presentation by Henrik Halen and Andreas Brinck presents global Illumination Based on Surfels ({GIBS}) – a solution for calculating indirect diffuse illumination in real-time.},
	titleaddon = {Electronic Arts Inc.},
	author = {Arts, Electronic},
	urldate = {2025-01-17},
	date = {2021-08-13},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Jarod\\Zotero\\storage\\7IFSY2AW\\siggraph21-global-illumination-surfels.html:text/html},
}

@video{path_of_exile_exilecon_2023,
	title = {{ExileCon} 2023 - Rendering Path of Exile 2},
	url = {https://www.youtube.com/watch?v=TrHHTQqmAaM},
	abstract = {A talk from Alexander Sannikov, a programmer at Grinding Gear Games. Recorded at {ExileCon} 2023.
You can find Alexander's {YouTube} channel here:     / @alexander\_sannikov  

The Shadertoy Alexander mentions during the talk can be found here: https://www.shadertoy.com/view/mlscz8

Alexander's article on Radiance Cascades can be found here: https://drive.google.com/file/d/1L6v1...},
	author = {{Path of Exile}},
	urldate = {2025-01-17},
	date = {2023-11-01},
}

@online{noauthor_acmsiggraph_nodate,
	title = {{ACMSIGGRAPH} - {YouTube}},
	url = {https://www.youtube.com/},
	abstract = {Profitez des vidéos et de la musique que vous aimez, mettez en ligne des contenus originaux, et partagez-les avec vos amis, vos proches et le monde entier.},
	urldate = {2025-01-17},
	langid = {french},
	file = {Snapshot:C\:\\Users\\Jarod\\Zotero\\storage\\R49V8YZT\\videos.html:text/html},
}

@article{ritschel_imperfect_nodate,
	title = {Imperfect Shadow Maps for Efﬁcient Computation of Indirect Illumination},
	abstract = {We present a method for interactive computation of indirect illumination in large and fully dynamic scenes based on approximate visibility queries. While the high-frequency nature of direct lighting requires accurate visibility, indirect illumination mostly consists of smooth gradations, which tend to mask errors due to incorrect visibility. We exploit this by approximating visibility for indirect illumination with imperfect shadow maps—low-resolution shadow maps rendered from a crude point-based representation of the scene. These are used in conjunction with a global illumination algorithm based on virtual point lights enabling indirect illumination of dynamic scenes at real-time frame rates. We demonstrate that imperfect shadow maps are a valid approximation to visibility, which makes the simulation of global illumination an order of magnitude faster than using accurate visibility.},
	author = {Ritschel, T and Grosch, T and Kim, M H},
	langid = {english},
	file = {PDF:C\:\\Users\\Jarod\\Zotero\\storage\\BVB2K4QA\\Ritschel et al. - Imperfect Shadow Maps for Efﬁcient Computation of Indirect Illumination.pdf:application/pdf},
}

@article{boisse_gi-10_nodate,
	title = {{GI}-1.0: A Fast Scalable Two-Level Radiance Caching Scheme for Real-Time Global Illumination},
	abstract = {Real-time global illumination is key to enabling more dynamic and physically realistic worlds in performance-critical applications such as games or any other applications with real-time constraints. Hardware-accelerated ray tracing in modern {GPUs} allows arbitrary intersection queries against the geometry, making it possible to evaluate indirect lighting entirely at runtime. However, only a small number of rays can be traced at each pixel to maintain high framerates at ever-increasing image resolutions.},
	author = {Boissé, Guillaume and Meunier, Sylvain and de Dinechin, Heloise and Oliver, Matthew and Bartels, Pieterjan and Veselov, Alexander and Eto, Kenta and Harada, Takahiro},
	langid = {english},
	file = {PDF:C\:\\Users\\Jarod\\Zotero\\storage\\4SGP6SL2\\Boissé et al. - GI-1.0 A Fast Scalable Two-Level Radiance Caching Scheme for Real-Time Global Illumination.pdf:application/pdf},
}

@online{noauthor_saschawillems_nodate,
	title = {{SaschaWillems} - Overview},
	url = {https://github.com/SaschaWillems},
	abstract = {{SaschaWillems} has 70 repositories available. Follow their code on {GitHub}.},
	titleaddon = {{GitHub}},
	urldate = {2025-01-17},
	langid = {english},
}

@online{shitpost_xxx69420xxx_real_2020,
	title = {Real time global illumination techniques?},
	url = {www.reddit.com/r/GraphicsProgramming/comments/jhh84i/real_time_global_illumination_techniques/},
	titleaddon = {r/{GraphicsProgramming}},
	author = {Shitpost{\textbackslash}\_xxx69420xxx},
	urldate = {2025-01-17},
	date = {2020-10-24},
	file = {Reddit Post Snapshot:C\:\\Users\\Jarod\\Zotero\\storage\\SNHJ3MAT\\real_time_global_illumination_techniques.html:text/html},
}

@article{sebastianaaltonenredlynxcom_gpu-driven_nodate,
	title = {{GPU}-Driven Rendering Pipelines},
	author = {sebastian.aaltonen@redlynx.com, ulrich haar@ubisoft com},
	langid = {english},
	file = {PDF:C\:\\Users\\Jarod\\Zotero\\storage\\F9BHHKVM\\sebastian.aaltonen@redlynx.com - GPU-Driven Rendering Pipelines.pdf:application/pdf},
}

@online{noauthor_rendering_2020,
	title = {Rendering Millions of Dynamic Lights in Real-Time},
	url = {https://developer.nvidia.com/blog/rendering-millions-of-dynamics-lights-in-realtime/},
	abstract = {Today {NVIDIA} is releasing the {ACM} {SIGGRAPH} 2020 research paper showing how to render dynamic direct lighting and shadows from millions of area lights in real-time. This was previously impossible.},
	titleaddon = {{NVIDIA} Technical Blog},
	urldate = {2025-01-17},
	date = {2020-05-14},
	langid = {american},
}

@inreference{noauthor_photon_2024,
	title = {Photon mapping},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Photon_mapping&oldid=1257901015},
	abstract = {In computer graphics, photon mapping is a two-pass global illumination rendering algorithm developed by Henrik Wann Jensen between 1995 and 2001 that approximately solves the rendering equation for integrating light radiance at a given point in space. Rays from the light source (like photons) and rays from the camera are traced independently until some termination criterion is met, then they are connected in a second step to produce a radiance value. The algorithm is used to realistically simulate the interaction of light with different types of objects (similar to other photorealistic rendering techniques). Specifically, it is capable of simulating the refraction of light through a transparent substance such as glass or water (including caustics), diffuse interreflection between illuminated objects, the subsurface scattering of light in translucent materials, and some of the effects caused by particulate matter such as smoke or water vapor.  Photon mapping can also be extended to more accurate simulations of light, such as spectral rendering. Progressive photon mapping ({PPM}) starts with ray tracing and then adds more and more photon mapping passes to provide a progressively more accurate render.
Unlike path tracing, bidirectional path tracing, volumetric path tracing, and Metropolis light transport, photon mapping is a "biased" rendering algorithm, which means that averaging infinitely many renders of the same scene using this method does not converge to a correct solution to the rendering equation.  However, it is a consistent method, and the accuracy of a render can be increased by increasing the number of photons. As the number of photons approaches infinity, a render will get closer and closer to the solution of the rendering equation.},
	booktitle = {Wikipedia},
	urldate = {2025-01-17},
	date = {2024-11-17},
	langid = {english},
	note = {Page Version {ID}: 1257901015},
}

@online{noauthor_siggraph_nodate,
	title = {{SIGGRAPH} Advances in Real-Time Rendering - {YouTube}},
	url = {https://www.youtube.com/@siggraphadvancesinreal-tim4519/videos},
	urldate = {2025-01-17},
}

@online{noauthor_photorealistic_nodate,
	title = {Photorealistic Rendering and the Ray-Tracing Algorithm},
	url = {https://pbr-book.org/3ed-2018/Introduction/Photorealistic_Rendering_and_the_Ray-Tracing_Algorithm},
	urldate = {2025-01-17},
}

@online{noauthor_introduction_nodate-1,
	title = {An Introduction to Physically Based Rendering},
	url = {http://typhomnt.github.io/teaching/ray_tracing/pbr_intro/},
	abstract = {Introduction to real-time physically based rendering.},
	titleaddon = {Maxime Garcia},
	urldate = {2025-01-17},
	langid = {english},
}

@online{noauthor_physically_nodate,
	title = {Physically Based Rendering: From Theory to Implementation},
	url = {https://www.pbrt.org/},
	urldate = {2025-01-17},
	file = {Physically Based Rendering\: From Theory to Implementation:C\:\\Users\\Jarod\\Zotero\\storage\\RS7VTVIC\\www.pbrt.org.html:text/html},
}

@online{noauthor_soulburn_nodate,
	title = {Soulburn Studios Art Lessons},
	url = {http://www.neilblevins.com/art_lessons/ggx/ggx.htm},
	urldate = {2025-01-17},
	file = {Soulburn Studios Art Lessons:C\:\\Users\\Jarod\\Zotero\\storage\\6ULR3NM7\\ggx.html:text/html},
}

@article{walter_microfacet_nodate,
	title = {Microfacet Models for Refraction through Rough Surfaces},
	abstract = {Microfacet models have proven very successful for modeling light reﬂection from rough surfaces. In this paper we review microfacet theory and demonstrate how it can be extended to simulate transmission through rough surfaces such as etched glass. We compare the resulting transmission model to measured data from several real surfaces and discuss appropriate choices for the microfacet distribution and shadowing-masking functions. Since rendering transmission through media requires tracking light that crosses at least two interfaces, good importance sampling is a practical necessity. Therefore, we also describe efﬁcient schemes for sampling the microfacet models and the corresponding probability density functions.},
	author = {Walter, Bruce and Marschner, Stephen R and Li, Hongsong and Torrance, Kenneth E},
	langid = {english},
	file = {PDF:C\:\\Users\\Jarod\\Zotero\\storage\\CVV4CIAH\\Walter et al. - Microfacet Models for Refraction through Rough Surfaces.pdf:application/pdf},
}

@article{allmenroder_linearly_nodate,
	title = {Linearly Transformed Spherical Harmonics},
	abstract = {Linearly transformed cosines approximate specular {BRDFs} in a way that enables efficient shading for Lambertian polygonal area lights. The approximation quality is mostly good but fails to reproduce tear shapes at grazing angle. We swap out the cosine with more general spherical harmonics ({SH}) expansions. Thus, our linearly transformed {SH} offers better fits at higher cost. Previous work allows us to integrate these {SH} expansions over polygons such that shading still works in closed form.},
	author = {Allmenröder, Jan and Peters, Christoph},
	langid = {english},
	file = {PDF:C\:\\Users\\Jarod\\Zotero\\storage\\LKRCJBZK\\Allmenröder and Peters - Linearly Transformed Spherical Harmonics.pdf:application/pdf},
}

@online{noauthor_simple_2014,
	title = {Simple and Fast Spherical Harmonic Rotation},
	url = {http://filmicworlds.com/blog/simple-and-fast-spherical-harmonic-rotation/},
	abstract = {Spherical harmonics rotation is one of those problems that you will occasionally run into as a graphics programmer.  There has been some recent work, most notably Sparse Zonal Harmonic Factorization for Efﬁcient {SH} Rotation (Project , {PDF}) which was presented at Siggraph 2012.  In games we usually care about low order {SH}, especially 3rd order.  According to Table 2, it takes 112 multiplications to rotate 3rd order {SH} by the zxzxz method, and 90 by the improved Zonal Harmonics method.  I’ll show you a simple way to do it in 57, and source is provided at the end of the post.},
	titleaddon = {Filmic Worlds},
	urldate = {2025-01-17},
	date = {2014-07-02},
	langid = {english},
}

@online{noauthor_graphics_nodate,
	title = {Graphics Programming Conference - {YouTube}},
	url = {https://www.youtube.com/},
	abstract = {Profitez des vidéos et de la musique que vous aimez, mettez en ligne des contenus originaux, et partagez-les avec vos amis, vos proches et le monde entier.},
	urldate = {2025-01-17},
	langid = {french},
}

@video{graphics_programming_conference_realtime_2024,
	title = {Realtime Global Illumination in Enshrouded},
	url = {https://www.youtube.com/watch?v=57F1ezwH7Mk},
	abstract = {Enshrouded has a voxel based environment, where nearly everything can be destroyed or built from scratch, so there is no room for pre-baked lighting. This presentation takes you on a journey through the development process of Enshrouded {GI} and shows which techniques worked for us or which didn’t.
Discover how we moved to our own {SDF} rays from Vulkan Raytracing to run on a wide range of {GPUs}. From shiny specular armor reflections, to diffuse foggy forests or deep dark caves, Enshrouded {GI} is capable of handling various situations dynamically and in real time. All rounded up with a bit of stochastic to make everything fast and smooth. Enshrouded {GI} shows what we achieved in a small team with our own handcrafted voxel engine.

\#gpc \#keengames},
	author = {{Graphics Programming Conference}},
	urldate = {2025-01-17},
	date = {2024},
}

@article{wronski_assassins_nodate,
	title = {Assassin’s Creed 4: Black Flag},
	author = {Wronski, Bartlomiej},
	langid = {english},
	file = {PDF:C\:\\Users\\Jarod\\Zotero\\storage\\LD55MS48\\Wronski - Assassin’s Creed 4 Black Flag.pdf:application/pdf},
}

@incollection{noauthor_deferred_2018,
	title = {Deferred Normalized Irradiance Probes John Huelin, Benjamin Rouveyrol, and Bart lomiej Wron´ski},
	isbn = {978-1-351-20123-0},
	abstract = {In this chapter we present deferred normalized irradiance probes, a technique
developed at Ubisoft Montreal for Assassin’s Creed 4: Black Flag. It was developed as a cross-console generation scalable technique and is running on all of our
six target hardware platforms: Microsoft Xbox 360, Microsoft Xbox One, Sony
Playstation 3, Sony Playstation 4, Nintendo {WiiU}, and {PCs}. We propose a partially dynamic global illumination algorithm that provides high-quality indirect
lighting for an open world game. It decouples stored irradiance from weather and
lighting conditions and contains information for a whole 24-hour cycle. Data is
stored in a {GPU}-friendly, highly compressible format and uses only {VRAM} memory. We present the reasoning behind a higher achieved quality than what was
possible with other partially baked solutions like precomputed radiance transfer
(under typical open-world game constraints).},
	booktitle = {{GPU} Pro 360 Guide to Lighting},
	publisher = {A K Peters/{CRC} Press},
	date = {2018},
	note = {Num Pages: 22},
}

@collection{engel_gpu_2015,
	location = {New York},
	title = {{GPU} Pro 6: Advanced Rendering Techniques},
	isbn = {978-0-429-16177-3},
	shorttitle = {{GPU} Pro 6},
	abstract = {The latest edition of this bestselling game development reference offers proven tips and techniques for the real-time rendering of special effects and visualization data that are useful for beginners and seasoned game and graphics programmers alike.Exploring recent developments in the rapidly evolving field of real-time rendering, {GPU} Pro6: Advance},
	pagetotal = {586},
	publisher = {A K Peters/{CRC} Press},
	editor = {Engel, Wolfgang},
	date = {2015-07-28},
	doi = {10.1201/b18805},
}

@article{speierer_metropolis_nodate,
	title = {Metropolis Virtual Point Light Rendering},
	abstract = {Solving the rendering equation is a well known problem in computer graphics and there exists plenty of methods to compute physically-based images. However, most of these methods aren’t fast enough to allow the user to interact with the scene in real-time.},
	author = {Speierer, Sebastien},
	langid = {english},
	file = {PDF:C\:\\Users\\Jarod\\Zotero\\storage\\BYRLIG9V\\Speierer - Metropolis Virtual Point Light Rendering.pdf:application/pdf},
}

@online{noauthor_engine_2020,
	title = {Engine Work: Global Illumination with Irradiance Probes},
	url = {https://handmade.network/p/75/monter/blog/p/7288-engine_work__global_illumination_with_irradiance_probes},
	shorttitle = {Engine Work},
	abstract = {Global illumination is one key effect I want to achieve with Monter. For a low-poly style game, G…},
	titleaddon = {Handmade Network},
	urldate = {2025-01-25},
	date = {2020-04-21},
	langid = {american},
	file = {Snapshot:C\:\\Users\\Jarod\\Zotero\\storage\\Y4ZRHQFM\\7288-engine_work__global_illumination_with_irradiance_probes.html:text/html},
}

@online{loggini_render_2021,
	title = {Render Graphs},
	url = {https://logins.github.io/graphics/2021/05/31/RenderGraphs.html},
	abstract = {Why Talking About Render Graphs In 2017 Yuriy O’Donnell, at the time working for Frostbite, presented the Frame Graph at {GDC}, which is considered the first application of render graph on triple A games. Frame Graph is intended to be a high-level representation of each graphics operation to render a scene. In addition to that, this system has full control over lifetime and usage of a big portion of render resources. As a consequence, this brings a series of advantages, such as a defined structure for all the rendering code, but also automatic and optimized resource transitions, sync fences and memory allocations. As of 2021 the use of a render graph has become a standard in {tripleA} games engine development: the most common example is the {RDG} from Unreal Engine and we know there are other implementations, such as the Frame Graph on the Anvil Engine from Ubisoft. Render graphs are also big and sometimes complex systems, and we can get the most advantage when dealing with a very big engine with many implemented features. Having it on a hobby or indie project might not bring all its full potential, since many optimizations can be done manually at that point, but it can always be very useful to understand how a {tripleA} engine works. Properties Having engine render code directed by a render graph brings many properties. First of all, it brings a much more clear order of execution to the graphics operations: each render operation gets abstracted to a single way to produce render code. The clarity of this structure allows for much more debuggability and the potential to build many display tools to show, for example, resource lifetime or render pass dependencies. This directly translates to less development time. It will also hide many low level operations, such as resource state transitions and memory allocations to the user, which leaves space to optimize such activities altogether in an independent and automatic way. All these properties will also help to raise the level of platform abstraction code in the engine, as many details and optimizations can be left hidden in platform-dependent code. Minimize And Batch Resource Barriers With the new generation graphics {API} such as Direct3D12 and Vulkan, we need to manage resource states and relative transitions depending on the operations we want to perform. By using render graphs, most of these operations can be handled automatically without any manual input from the programmer, which helps to reduce errors, makes the code cleaner and more focus can be dedicated to developing the actual graphics features. For each render pass the programmer declares what resources will be used as shader input, render targets and depth-stencils (more details on this in the Composition and workflow section). This is a big advantage as we can deduce the resource transitions that each pass requires. The image above shows a visual example of resource usage: green lines are read operations and red lines are write operations on different resources for each render pass. Since the render graph has knowledge of these interactions, it can also figure out the placing of barriers for resource transitions. Detecting the optimal barrier configuration on a single command queue is quite trivial: if a resource A is used as shader resource for pass 1 but as a render target for pass 2, then we will need a resource transition to render target between the two passes. If both pass 1 and 2 use resource A as a shader resource, then there is no need to perform any transition. In general, the more we can group resource transitions in a single call, the less calls to the {SDK} and the faster our program will perform. Let’s take the example of a shadow mapping algorithm: we will have a graphics pass that computes a depth buffer, then a per-pixel compute pass that uses that depth information to output a shadow buffer, and finally another graphics pass that uses both the depth buffer and shadow buffer as input. In this case, the transition of both depth buffer and shadow buffer can (and should) be grouped together when they are about to get used as shader resources for the last pass. We can batch (group) resource barriers even if used resources are coming from different command queues. In the case of a {tripleA} engine for example, it is common to have at least 3 command queues: a graphics, a compute and a copy one. The graphics command list is meant to be the “main” queue, and in general all the queues will need synchronization mechanisms (specifically {GPU} fences in D3D12) to communicate with each other. The optimization of resource transitions between different command queues can become complicated depending on the interaction between passes and how many different queues we are using. Pavlo Muratov in his blog post explains the difficulties of making these operations fully automatic and lists the approaches we can take when dealing with an indefinite number of queues. In general it looks like the better option is grouping resource transitions and executing them on a single queue. Manage And Optimize Resource Memory Since each pass declares which resources are needed, it is also possible to control the lifetime of resources: allocations are also handled by the graph. In the image above, for example, resource A is used only up to the third pass. On the other hand, resource C starts getting used in the fourth pass, and so its lifetime does not overlap the one of resource A, meaning that we can use the same memory for both resources. The same concept applies for resource A and D, and in general we will have multiple ways to overlap our memory allocations, so we will also need clever ways to detect the best allocation strategy. There will be resources which will be used on a per-frame basis: these are commonly called graph or transient resources and their lifetime can be fully handled by the render graph. An example is the camera depth or the Gbuffers in a deferred lighting pass, as they will be computed on a per-frame basis. There are then other resources, such as the window swapchain back buffer, whose lifetime is dependent on systems outside the graph, and so the graph will limit itself to just manage their state. These are known as external resources. Transient Resource System Since all the resources created within the {RenderGraph} are meant to last for a specific time span within a single frame, there is a high potential for memory re-use. These resources, owned by the render graph and lasting a maximum of one frame, are also called transient resources. The resource lifetime is then used to apply resource aliasing on placed resources (by strictly using D3D12 terminology). I have talked about placed resources and resource aliasing in my previous article about Resource Handling in D3D12. Placed resources are faster to allocate and remove compared to reserved resources since they reside on an already created {GPU} memory heap. Along with them, we often find the concept of resource aliasing, which consists of using the same {GPU} memory for different resources whose lifetime don’t overlap during frame computation. More details about placed resources and aliasing in the Available D3D Tools chapter. Regarding buffers containing per-object data (and not images), this system is not necessary due to the small size that is being taken per frame, and we can use a linear allocator. For textures the situation is different, since they take most of the used {GPU} memory, and so we can use strategies of resource aliasing with them. I will further explain resource aliang in the Available Direct3D Tools section. If we are interested in optimizing performance, a blog post by Pavlo Muratorv explains an algorithm to find the optimal scheduling for aliased resources in a render graph. Parallel Command List Recording As mentioned before, having a render graph also helps handling pass connections between different command queues running at the same time. The most common case is having a Graphics + Async Compute command queues configuration. The command to run a compute pass asynchronously should be just a matter of calling {MyGraphBuilder}::{ExecuteNextAsync}(); just before the pass definition. The graph then will take this information to kick off the next pass on the parallel compute queue instead of using the graphics one. For an even smarter graph, this option can be decided by inspecting the flow structure. We could make the graph automatically handle async compute operations but turns out this is not optimal and manual settings are preferred. For every pass that we run asynchronously, we need to make sure that resource lifetime will last up until the pass output will be used again on the main render thread. All these mechanics are handled using {GPU} fences, as we are essentially synchronizing different thread executions. The tricky part will consist in finding the best approach to minimize the number of such syncing points, as they have a timing cost. You can find more information about D3D12 command queues synchronization in the official documentation. Composition And Workflow I briefly talked about the render graph used by {UE}4 in a previous blog post about its shaders, and how it works. A render graph uses a Retained Mode Model, which consists in first gathering all the information to render a frame, then optimize the workflow and finally render the result. This directly opposes “Immediate Mode” where elements get rendered as they get first considered. Despite immediate mode being the most straightforward to implement, the use of retained mode allows much more optimization. We can identify 3 steps when using a render graph: Setup phase: declares which render passes will exist and what resources will be accessed by them. Compile phase: figure out resources lifetime and make resource allocations accordingly. Execute phase: all the graph nodes get executed. Frostbite Frame Graph gets built from scratch on every frame and it has a code-driven architecture. It was a conscious choice to allow more flexibility, e.g. removing/adding passes upon need. We do not really need to build the entire graph on every frame, as discussed in apoorvaj.io, but only when the graph changes. A strategy can be computing a hash for the declared combination of passes just after the setup phase. In the next frame, if the new computed hash is the same as the one computed in the previous frame, we will reuse the already computed graph, otherwise we are going to build a new one. Note: we could even decide to make the graph static, in a system that runs the graph setup only once, and rebuild it only if explicitly requested, if we are sure that our system will almost always need all the declared passes. To build a graph we can use a {GraphBuilder} type object, from which we can statically retrieve an instance used to push all the render passes. {GraphBuilder}::Get().{AddPass}( “{MyPassName}”, Flags, {MyPassParameters}, [{MyPassParameters}]()\{ /* My lambda function body where we will use the {PassParameters} (and possibly other information) to send dispatch or draw commands */ \} ); Each pass will be created from: Name Flags (e.g. if the pass is to be considered a graphics or compute one) Pass parameters: used to understand the needed resource transitions, lifetimes and aliasing. Execution lambda: where we are still going to capture the parameters just declared to use them on the drawing or dispatch commands. The {PassParameters} will need to distinguish between at least shader resources and render targets in order to detect proper transitions. struct {MyPassParameterStruct}\{ std::vector{\textless}{ResourceRef}{\textgreater} {ShaderResources}; std::vector{\textless}{ResourceRef}{\textgreater} {RenderTargets}; \} These render passes can be further organized into Render Features, groups of passes that define a specific effect applied to the render pipeline. Examples of render features can be Base Pass, Environment Reflections, Deferred Lighting or Shadow Maps. It can happen that a feature resource input depends on another one, e.g. deferred lighting depends on base pass, or shadow maps depend on shadow depths. For this reason we will also need a system to handle such dependencies. One solution is using a map container to store output data reference of each feature: the dependent pass will include the header where the output of the first pass is defined, and add its resources as dependency where needed. // At the end of a feature an instance of this struct will be filled and inserted in the output map struct {MyFirstFeatureOutput} \{ {ResourceRef} {MyFirstOutput}; \} // … {MyFirstFeatureOutput} {featureOutput} \{ {outputResource} \}; {GraphBuilder}::{AddFeatureOutput}{\textless}{MyFirstFeatureOutput}{\textgreater}({featureOutput}); // Then in code of a pass of the dependent feature \#include “{MyFirstFeature}.h” {MyFirstFeatureOutput}\& {firstFeatOutput} = {GraphBuilder}::{GetFeatureOutput}{\textless}{MyFirstFeatureOutput}{\textgreater}(); {MyPassParameterStruct}.{ShaderResources}.add({firstFeatOutput}.{MyFirstOutput}); Another solution, like the one adopted by {UE}4, is retrieving resources from the graph by hashed string. {ResourceRef} {sceneColor} = {GraphBuilder}::{GetOrCreateResource}("{SceneColor}"); {MyPassParameterStruct}.{ShaderResources}.add({sceneColor}); Of course this method is less safe, because we rely on knowing what string will be used for what resource in another pass. On the other hand, this approach can be more flexible as we do not expect the output of a specifc pass, but just a named resource which, if not used before, will be created on the spot. Setup Phase In the setup phase the system will go through all the defined render passes and check all the declared resources. Each declared resource will have a flag associated for its usage type, e.g. Read, Write, {ComputeRW}, Render Target, {DepthStencil} and also communicate the graph when we want to allocate new graph resources, e.g. {TextureRef} {MyGraphBuilder}::{CreateTexture}(const {TextureDesc}\& Desc, const char* Name, {TextureFlags} Flags); Referenced (or created) resources can specify {TextureFlags} such as Clear, if we want to fill the texels with a certain value before use in a pass, in order to avoid reading garbage data. The {CPU} memory of these resources will be guaranteed only for the execution of the pass and at the end of it (both {CPU} and {GPU}) the memory will be considered re-usable for the next frames. Regarding render target resources, whenever we execute a write on one of them, we will need to invalidate all its previous references in the graph, in order to avoid having passes executing after the current one, and thinking of using the previous state of the resource. This can be achieved by assigning a different internal update index to the resource, so it can be checked for validity at runtime. Aside from resources which lifetime is tied to the execution of the graph, we can also reference external resources, the ones created from external systems (because used in different ways or also needed for other sides of the engine) {BufferRef} {MyGraphBuilder}::{RegisterExternalBuffer}(const {PooledBuffer}\& {ExternalPooledBuffer}, {BufferFlags} Flags); In such cases the resource will be tracked by the graph, executing state transitions when needed. An example of an external resource is the window swap chain backbuffer of Windows {API}. Usually we can rely on the order of which passes get registered for the graph, to have a reliable timeline for pass execution on a single command queue. As a reminder, commands sent to a single command queue are guaranteed to get executed in the order we send them. Further Optimization Graph resources can be created with lazy initialization, so creating them within graph setup but allocating them only before the first pass that needs them. The system can also deduce the state of used resources, e.g. a texture bound as a resource for a pixel shader can be deduced as read state, otherwise specified (like if we want to use it with an unordered access view). It is also possible to deduce creation of a resource from a pass input, for example deducing the size and attributes when creating a mipmapped texture from an input buffer image. Compile Phase The compile phase is completely autonomous and a “non-programmable” stage, in the sense that the render pass programmer does not have influence on it. In this phase the graph gets inspected to find all the possible flow optimizations, it will: Exclude unreferenced but defined resources and passes: if we want to draw a second debug view of the scene, we might be interested to draw only certain passes for it. Compute and handle used resources lifetime Resources Allocation Build optimized resource transition graph For optimized resource transitions, in D3D12 we have the {ID}3D12GraphicsCommandList::{ResourceBarrier} method intended here for the specific case of D3D12\_RESOURCE\_TRANSITION\_BARRIER. With this concept in mind we can build an internal subsystem that handles resource transitions for us. A very basic example (still without using a render graph) can be found in the section Resource State Tracking of Learning {DirectX} 12 - Part3 by 3dgep.com, and so here will not be discussed. When more than one command queue is running, the situation adds a layer of complexity. Build Cross-Queues Synchronization As you may already know, render operations in the same command queue are executed sequentially, but in the case that we are using multiple command queues, they will run in parallel, and so we need synchronization mechanisms to prevent race conditions on shared resources. The less fencing we use, the faster our render operations will be, and the compile phase is a good place to insert an algorithm to minimize the wait between two or more queues. Note: Unreal Engine, and also Frostbite when presented render graphs in 2017, do not compute the minimal amount of fences, and put manual fences where they want to optimize workflow or when needed in special cases, because in a real {tripleA} engine scenario, there are many hidden edge cases that might make a single algorithm to be impractical to maintain. Pavlo Muratov in his blog article explains an idea on how to compute automatic optimal fences between parallel command queues. One of the main concepts presented is the fact that in {APIs} such as D3D12, the graphics command queue is the only one that can transition all the possible kind of resource states, and so it can be called “the most competent” queue. For this reason, it is proven to be better to execute all the resource transitions on a single (graphics) queue, after proper synchronization. In his article, Pavlo describes how to build a dependency tree starting from an acyclic graph of render passes, which we have after we lay down every pass from the dependent queues. Each level of a dependency tree, called dependency level, will contain passes independent of each other, in the sense of their resource usage. By doing that we can ensure that every pass in the same dependency level can potentially run asynchronously. We can still have cases of multiple passes belonging to the same queue in the same dependency level, but this does not bother us. As a consequence, we can put a synchronization point with a {GPU} fence at the end of every dependency level: this will execute the needed resource transitions, for Every queue on a single graphics command list. This approach of course does not come for free, since using fences and syncing different command queues has a time cost. In addition to that, it will not always be the optimal and smallest amount of synchronizations, but it will produce acceptable performance and it should cover all the possible edge cases. Note: In real use cases, the amount of parallelization we can achieve depends on the current hardware (which many details are hidden from the programmer) and also may vary depending on how much workload we have for each pass. Execute Phase Execution phase is as simple as navigating through all the passes that survived the compile phase culling and executing the draw and dispatch commands on the list. Up until the execute phase all the resources were handled by opaque and abstract references, while at execute phase we access the real {GPU} {API} resources and set them in the pipeline. The preparation of command lists, on the {CPU} side, can be potentially parallelized quite a lot: in most of the cases, each pass command list setup is independent from each other. Aside from that, command list submissions on a single command queue is not thread safe, and in any case we would first need to determine if adding parallelization would bring significant gains. Available Direct3D Tools Submit Command Lists We can submit multiple command lists at the same time by using {ID}3D12CommandQueue::{ExecuteCommandLists} method. For example, if we build a pass dependency tree first, where each layer contains independent passes. We can put every pass on the same “dependency layer” on a different command list, and then execute all together with the same {ExecuteCommandLists} call. The trick here is that we will never be sure how much of these operations will be actually made parallel on {GPU}. The documentation itself specifies that the work of a second list “may start” before the end of the first one of the batch, but we will never know for sure, as these are hardware-dependent details! What we know for sure is that in the case we need to submit multiple command lists, we will spare the {CPU} workload of submitting them singularly. Placed Resources Since creating Committed resources, and so having an independent resource heap for each of them, would use ton of memory, the straightforward solution to it is to use Placed Resources. They are called “placed” because they will be allocated linearly in a pre-existing, and ideally permanent, {GPU} memory heap. This characteristic makes them the fastest and most flexible type or resource to handle, which was not available in Direct3D11. We can create them by calling {ID}3D12Device::{CreatePlacedResource} which asks for an already existing heap and offset, plus details about the resource we want to allocate. When handling placed resources, we can operate memory aliasing with them, which is described next. Aliased Resources The fundamental concept for aliased resources is if, during the render timeline, two resources have a usage (and so a lifetime) that does not overlap, then they can use the same memory. Note: Aliasied resources can spare more than 50\% of the used resource allocation space, especially when using a render graph. They add an additional managing resource complexity to the scene, but if we want to spare memory, they are almost always worth it. We can also alias reserved resources, also known as tiled resources, but this goes beyond the scope of this article. For the simple model usage of aliased resources, we say that an aliased resource is** active, when the shared memory is currently assigned for such resource use. In the meantime, all the other resources that share memory with the previous one, will be considered **inactive, and so not meant to be used. It is considered invalid for a {GPU} to read or write on an inactive resource and placed resources are created in an inactive state. Between the usage of two overlapping aliased resources we always need an aliasing barrier. They are defined by the D3D12\_RESOURCE\_ALIASING\_BARRIER structure. Using them is similar to resource transition barriers except they are made for aliased resources. When we are dealing with aliasing resources, regular resource barriers are no longer useful since they would refer to a place of memory shared by multiple resources. D3D12\_RESOURCE\_ALIASING\_BARRIER {myAliasingBarrier}\{ {prevResource}.Get(), {resourceToActivate}.Get()\}; {myCmdList}-{\textgreater}{ResourceBarrier}(1, \&{myAliasingBarrier}); The aliasing barrier takes as arguments the resource that existed before aliasing and the resource that will become the active resource after the barrier. The before resource can be left {NULL}, and in that case all the resources that shared memory with the after resource, will become inactive. It is possible to batch aliasing barriers with other transition barriers and we should do it to achieve best performance. As explained in a blog post by Adam Sawiki, there is an important detail concerning resources created with D3D12\_RESOURCE\_FLAG\_ALLOW\_RENDER\_TARGET or D3D12\_RESOURCE\_FLAG\_ALLOW\_DEPTH\_STENCIL from D3D12\_RESOURCE\_FLAGS enumeration. Render targets and depth stencils get handled differently by the {GPU}, compared to shader resources such as textures, and they need additional care when used in resource aliasing. Specifically, every time they become the active aliased resources, and so after an aliasing barrier that sees them as after resources, they need to be initialized with one of the following operations: A Clear operation: like {ClearRenderTargetView} or {ClearDepthStencilView} which will fill the current resource with a specified value, such as 1 or 0 or a defined color. A clear comes handy when the next render operations will be changing just a subset of the current resource data, and we need the remaining to contain valid information. Examples include filling a render target with blue color before drawing a cube into it. A {DiscardResource} operation: a very quick operation that updates just the resource metadata by informing that the current content does not need to be preserved. {DiscardResource} is the preferred choice when the next operation is going to compute every texel of our resource. A Copy operation: such as {CopyBufferRegion}, {CopyTextureRegion}, or {CopyResource} will copy the content of another resource into the current one. Note: For non-render targets and non-depth-stencil the initialization condition is not mandatory, and we are free to read garbage data from them! We can also use aliased resources with an advanced use semantic, explained in the official docs, to get the best performance in specific situations. With advanced use we can forget about resource active and inactive states, and access subregions of overlapping resources at any time, as long as the following rules are satisfied: We still need an aliasing barrier between two different {GPU} resource accesses on the same memory subregion. We still need subregion initialization for render targets and depth-stencils like it was happening in the simple model. Advices Fences Are Expensive Fences in a render graph are mainly used to synchronize the access of resources, especially when shared between two command queues (e.g. graphics + compute). Using them finely grained for each resource is bad: we should signal a fence to sync as many resource usages as possible. As mentioned in previous sections, it is possible to build a deferred barrier system, where barriers are appended to a list and wait up until necessary before flushing the list. With a single command queue this system is quite straightforward but with multiple queues it can become complex very quickly, as already explained in the Compile Phase section. Use Resource Shader Visibility Shader visibility is a parameter of D3D12\_ROOT\_PARAMETER structure that will be used as input for the root signature. By declaring the right shader visibility on resource views and samplers, since the graphics drivers can optimize their usage. This should come along with the render graph functionalities: since the graph has the full list of passes on which a specific resource will be used and in which ways, it is also possible to decide the proper shader visibility. Do not use {SHADER}\_VISIBILITY\_ALL as it is the less optimized entry. Modify Root Signatures and {PSOs} is Expensive Cache root signatures and {PSOs} as much as possible, and reduce changes to such objects to the minimum. You can read more about {PSO} and Root Signature in my previous articles:Pipeline State Object and Root Signature. Caching {PSOs} can restrict the range of possible operations on a material: changing depth bias, viewport, scissor rect, rasterizer or any other part of the {PSO} would invalidate cache and so we cannot allow these operations to be done per frame anymore. In the case of depth bias for example, we can solve this issue by considering the bias inside the pixel shader instead of using the {PSO} parameter. {PSO} cache can take memory, and we can potentially have a high number of different {PSOs} for each material permutation, but this is an acceptable cost to gain runtime performance. Descriptor tables can be built from shader parameters groups that do not change frequently. An example are static descriptors for a material shader parameter group: we can first build a root table entry referencing them, then reference the same root table entry every time we need to use such material. Sources Holger Gruen - {DirectX}™ 12 Case Studies https://developer.download.nvidia.com/assets/gameworks/downloads/regular/{GDC}17/{DX}12CaseStudies\_GDC2017\_FINAL.pdf Yuriy O’Donnell - {FrameGraph} in Frostbite https://www.gdcvault.com/play/1024612/{FrameGraph}-Extensible-Rendering-Architecture-in Tiago Rodrigues - Moving To {DirectX} 12 https://www.gdcvault.com/play/1024656/Advanced-Graphics-Tech-Moving-to Our Machinery - High-Level Rendering Using Render Graphs https://ourmachinery.com/post/high-level-rendering-using-render-graphs/ Graham Wihlidal - Halcyon: Rapid Innovation using Modern Graphics https://media.contentapi.ea.com/content/dam/ea/seed/presentations/wihlidal2019-rebootdevelopblue-halcyon-rapid-innovation.pdf Apoorva Joshi - Render Graphs https://apoorvaj.io/render-graphs-1/ Pavlo Muratov - Organizing {GPU} Work with Directed Acyclic Graphs https://levelup.gitconnected.com/organizing-gpu-work-with-directed-acyclic-graphs-f3fd5f2c2af3 Pavlo Muratov - {GPU} Memory Aliasing https://levelup.gitconnected.com/gpu-memory-aliasing-45933681a15e Adam Sawicki - Initializing {DX}12 Textures After Allocation and Aliasing https://asawicki.info/news\_1724\_initializing\_dx12\_textures\_after\_allocation\_and\_aliasing Microsoft - Memory Aliasing and Data Inheritance https://docs.microsoft.com/en-us/windows/win32/direct3d12/memory-aliasing-and-data-inheritance Microsoft - {ID}3D12Device::{CreatePlacedResource} https://docs.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource Microsoft - Using Resource Barriers https://docs.microsoft.com/en-us/windows/win32/direct3d12/using-resource-barriers-to-synchronize-resource-states-in-direct3d-12},
	titleaddon = {Riccardo Loggini},
	author = {Loggini, Riccardo},
	urldate = {2025-02-11},
	date = {2021-05-31},
	langid = {american},
	file = {Snapshot:C\:\\Users\\Jarod\\Zotero\\storage\\L3ATYP2K\\RenderGraphs.html:text/html},
}

@online{htuhola_how_2017,
	title = {How do you abstract details in render passes, framebuffers and the pipeline?},
	url = {www.reddit.com/r/vulkan/comments/635a9u/how_do_you_abstract_details_in_render_passes/},
	titleaddon = {r/vulkan},
	author = {htuhola},
	urldate = {2025-02-11},
	date = {2017-04-03},
	file = {Reddit Post Snapshot:C\:\\Users\\Jarod\\Zotero\\storage\\Y866PAI2\\how_do_you_abstract_details_in_render_passes.html:text/html},
}

@video{cppcon_cppcon_2017,
	title = {{CppCon} 2017: Nicolas Guillemot “Design Patterns for Low-Level Real-Time Rendering”},
	url = {https://www.youtube.com/watch?v=mdPeXJ0eiGc},
	shorttitle = {{CppCon} 2017},
	abstract = {http://{CppCon}.org
—
Presentation Slides, {PDFs}, Source Code and other presenter materials are available at: https://github.com/{CppCon}/{CppCon}2017
—
In recent years, the {GPU} graphics community has seen the introduction of many new {GPU} programming {APIs} like Khronos' Vulkan, Microsoft's Direct3D 12, and Apple's Metal. These {APIs} present much more control of {GPU} hardware, but bring with them a great increase in complexity. We need to rethink the way we do graphics programming to take advantage of new features, while also keeping complexity under control. 

This talk presents solutions to recurring programming problems with these new {GPU} graphics {APIs}. These solutions are intended to simplify the complexity of the {API} by an order of magnitude, while simultaneously improving overall performance. This talk aims to discuss some key techniques for other developers to create their own {GPU} rendering engine. 

Topics covered include using a ring buffer to stream data and descriptors from {CPU} to {GPU}, scheduling {GPU} memory and work from the {CPU}, designing a multi-pass real-time {GPU} renderer, and using fork/join parallelism to increase the performance of the {CPU} code that submits {GPU} work.
— 
Nicolas Guillemot: University of Victoria, {MSc} Student

Nicolas is a master's student at the University of Victoria, where he searches for solutions to the game industry's real-time rendering problems at the intersection of software and hardware. In the past, Nicolas has worked at Electronic Arts, and in Intel's Visual and Parallel Computing Advanced Technology Group.
—
Videos Filmed \& Edited by Bash Films: http://www.{BashFilms}.com 

*-----*
Register Now For {CppCon} 2022: https://cppcon.org/registration/
*-----*},
	author = {{CppCon}},
	urldate = {2025-02-11},
	date = {2017-10-12},
}

@video{rendering_engine_architecture_conference_siggraph_2021,
	title = {{SIGGRAPH} 2021 Rendering Engine Architecture in Games, Welcome and Introduction},
	url = {https://www.youtube.com/watch?v=HtH1GMnZpgo},
	abstract = {Welcome and introduction to the inaugural {SIGGRAPH} 2021 Rendering Engine Architecture in Games course (see http://enginearchitecture.realtimeren... for more). 

Modern video games employ a variety of sophisticated algorithms to produce groundbreaking 3D rendering pushing the visual boundaries and interactive experience of rich environments. These algorithms execute in a variety of rendering engines, which serve diverse needs, from a wide range of frame rates (30-200 fps), on a deeply fragmented hardware ecosystem. The goal for this course is to share pragmatic presentations about the design decisions and architectures for the real-time game engines, with the emphasis on production-proven approaches. 

This is the course to attend if you are in the game development industry or want to learn the latest and greatest techniques in the real-time rendering domain!

Prerequisites
 
Working knowledge of modern real-time graphics {APIs} like {DirectX}, Vulkan or Metal, and a solid basis in the basics of rendering engine architecture. Familiarity with the concepts of programmable shading and shading languages. Familiarity with shipping titles on gaming consoles and their software capabilities is a plus but not required.
Intended Audience
 
Technical practitioners and developers of graphics engines for visualization, games, or effects rendering who are interested in interactive rendering.},
	author = {{Rendering Engine Architecture Conference}},
	urldate = {2025-02-11},
	date = {2021-10-30},
}

@video{rendering_engine_architecture_conference_reac_2023,
	title = {{REAC} 2023 {DAY} 1 Task Graph Renderer at Activision},
	url = {https://www.youtube.com/watch?v=pr8HaIaZfpk},
	abstract = {Render graphs have become an established way of handling the complexities of synchronization, memory allocation and scheduling for modern rendering engines. In this talk we’ll cover our task graph renderer, paying particular attention to the design of the engineer facing {APIs}, as well as some of the backend implementation details that allow us to scale our renderer from mobile all the way up to the 9th generation of consoles and beyond.

Speakers:

Charlie Birtwistle is a Senior Principal Engineer at Activision Central Tech, where he works on the low-level rendering architecture and {APIs} that power Activision’s games. He started in the industry nearly 20 years ago working on launch titles for Xbox 360.

Francois Durand is a Technical Director at Beenox, an Activision studio. Having worked almost 15 years in the industry on franchises like The Amazing Spider-Man, Skylanders and Call of Duty, he now focuses on finding ways to improve graphics and engine scalability.},
	author = {{Rendering Engine Architecture Conference}},
	urldate = {2025-02-11},
	date = {2023},
}

@video{ndc_conferences_adventures_2023,
	title = {Adventures in Rendering Off the Main Thread - Simon {MacDonald} - {NDC} Oslo 2023},
	url = {https://www.youtube.com/watch?v=FgGHquE_dYA},
	abstract = {When building out the frontend for an application with strong real-time requirements, there are many considerations to make. How do we get initial data? How do we get subsequent updates? What happens if they lose connectivity? Web components provide an excellent model for progressively enhancing initial markup. Workers allow us to move, rendering off the main thread. In this talk, we'll demonstrate an architecture that scales down to any device with a spotty internet connection while scaling up to a fully realized real-time application.

Check out our new channel: 
{NDC} Clips:
‪@ndcclips‬
 

Check out more of our featured speakers and talks at
https://ndcconferences.com/
https://ndcoslo.com/},
	author = {{NDC Conferences}},
	urldate = {2025-02-11},
	date = {2023},
}

@video{unity_introduction_2024,
	title = {Introduction to the Render Graph in Unity 6},
	url = {https://www.youtube.com/watch?v=U8PygjYAF7A},
	abstract = {Render Graph helps Unity's Render Pipeline optimize runtime rendering--that better performance provides broader and safer access for creating your own features. In this tutorial we create a dither effect Renderer Feature by using a Full Screen Shader Graph material with render graph's optimized resource management. We also get to know the Render Graph Viewer and explore what it tells us about the pipeline.

⭐ Learn more about render graph or {URP} Unity 6 from our latest e-book for advanced creators: https://unity.com/resources/introduct... 

Note about 15:32: As an alternative to using the custom function node to get the luminance, you can also use the dot product node with (0.299, 0.587, 0.114).

The project shown in this video and e-book can be found here: https://github.com/{NikLever}/Unity6E-book

⭐ Check out the Renderer feature section in the Unity Documentation: https://docs.unity3d.com/6000.0/Docum... 

Time Stamps: 
0:00 Intro
1:06 Render Graph Viewer
5:45 Dither Effect Feature
13:00 Volume Setup
14:32 Fullscreen Shader Graph
17:21 More Effects!},
	author = {{Unity}},
	urldate = {2025-02-11},
	date = {2024-11-04},
}

@online{noauthor_task_nodate,
	title = {Task Graph {\textbar} Our Pattern Language},
	url = {https://patterns.eecs.berkeley.edu/?page_id=609},
	urldate = {2025-02-11},
	langid = {american},
}

@online{noauthor_raycasting_nodate,
	title = {Raycasting},
	url = {https://lodev.org/cgtutor/raycasting.html},
	urldate = {2025-03-18},
	file = {Raycasting:C\:\\Users\\Jarod\\Zotero\\storage\\DN9TZHU3\\raycasting.html:text/html},
}

@video{victor_gordan_computer_2021,
	title = {Computer Graphics Tutorial - {PBR} (Physically Based Rendering)},
	url = {https://www.youtube.com/watch?v=RRE-F57fbXw},
	author = {{Victor Gordan}},
	urldate = {2025-03-18},
	date = {2021-09-05},
}

@video{first_principles_of_computer_vision_brdf_2021,
	title = {{BRDF}: Bidirectional Reflectance Distribution Function},
	url = {https://www.youtube.com/watch?v=R9iZzaXUaK4},
	shorttitle = {{BRDF}},
	author = {{First Principles of Computer Vision}},
	urldate = {2025-03-18},
	date = {2021-03-14},
}

@video{udacity_brdf_2015,
	title = {{BRDF} - Interactive 3D Graphics},
	url = {https://www.youtube.com/watch?v=4QXE52mswIE},
	author = {{Udacity}},
	urldate = {2025-03-18},
	date = {2015},
}

@video{simondev_exploring_2024-1,
	title = {Exploring a New Approach to Realistic Lighting: Radiance Cascades},
	url = {https://www.youtube.com/watch?v=3so7xdZHKxw},
	shorttitle = {Exploring a New Approach to Realistic Lighting},
	author = {{SimonDev}},
	urldate = {2025-03-18},
	date = {2024},
}

@online{spektre_answer_2021,
	title = {Answer to "What is the difference between ray tracing, ray casting, ray marching and path tracing?"},
	url = {https://stackoverflow.com/a/67354199},
	shorttitle = {Answer to "What is the difference between ray tracing, ray casting, ray marching and path tracing?},
	titleaddon = {Stack Overflow},
	author = {Spektre},
	urldate = {2025-03-18},
	date = {2021-05-02},
	file = {Snapshot:C\:\\Users\\Jarod\\Zotero\\storage\\P97UIS7B\\what-is-the-difference-between-ray-tracing-ray-casting-ray-marching-and-path-t.html:text/html},
}

@video{justin_solomon_introduction_2020,
	title = {Introduction to Computer Graphics (Lecture 16): Global illumination; irradiance/photon maps},
	url = {https://www.youtube.com/watch?v=odXCvJTNn6s},
	shorttitle = {Introduction to Computer Graphics (Lecture 16)},
	author = {{Justin Solomon}},
	urldate = {2025-03-18},
	date = {2020},
}

@misc{osborne_radiance_2024,
	title = {Radiance Cascades: A Novel High-Resolution Formal Solution for Multidimensional Non-{LTE} Radiative Transfer},
	url = {http://arxiv.org/abs/2408.14425},
	doi = {10.48550/arXiv.2408.14425},
	shorttitle = {Radiance Cascades},
	abstract = {Non-{LTE} radiative transfer is a key tool for modern astrophysics: it is the means by which many key synthetic observables are produced, thus connecting simulations and observations. Radiative transfer models also inform our understanding of the primary formation layers and parameters of different spectral lines, and serve as the basis of inversion tools used to infer the structure of the solar atmosphere from observations. The default approach for computing the radiation field in multidimensional solar radiative transfer models has long remained the same: a short characteristics, discrete ordinates method, formal solver. In situations with complex atmospheric structure and multiple transitions between optically-thick and -thin regimes these solvers require prohibitively high angular resolution to correctly resolve the radiation field. Here, we present the theory of radiance cascades, a technique designed to exploit structure inherent to the radiation field, allowing for efficient reuse of calculated samples, thus providing a very high-resolution result at a fraction of the computational cost of existing methods. We additionally describe our implementation of this method in the {DexRT} code, and present initial results of the synthesis of a snapshot of a magnetohydrodynamic model of a solar prominence formed via levitation-condensation. The approach presented here provides a credible route for routinely performing multidimensional radiative transfer calculations free from so-called ray effects, and scaling high-quality non-{LTE} models to next-generation high-performance computing systems with {GPU} accelerators.},
	number = {{arXiv}:2408.14425},
	publisher = {{arXiv}},
	author = {Osborne, Christopher M. J. and Sannikov, Alexander},
	urldate = {2025-03-21},
	date = {2024-08-26},
	eprinttype = {arxiv},
	eprint = {2408.14425 [astro-ph]},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics},
	file = {Full Text PDF:C\:\\Users\\Jarod\\Zotero\\storage\\7QF67CPA\\Osborne et Sannikov - 2024 - Radiance Cascades A Novel High-Resolution Formal Solution for Multidimensional Non-LTE Radiative Tr.pdf:application/pdf;Snapshot:C\:\\Users\\Jarod\\Zotero\\storage\\M5CVBIGE\\2408.html:text/html},
}

@inproceedings{mcguire_real-time_2017,
	location = {San Francisco California},
	title = {Real-time global illumination using precomputed light field probes},
	isbn = {978-1-4503-4886-7},
	url = {https://dl.acm.org/doi/10.1145/3023368.3023378},
	doi = {10.1145/3023368.3023378},
	abstract = {We introduce a new data structure and algorithms that employ it to compute real-time global illumination from static environments. Light ﬁeld probes encode a scene’s full light ﬁeld and internal visibility. They extend current radiance and irradiance probe structures with per-texel visibility information similar to a G-buffer and variance shadow map. We apply ideas from screen-space and voxel cone tracing techniques to this data structure to efﬁciently sample radiance on world space rays, with correct visibility information, directly within pixel and compute shaders. From these primitives, we then design two {GPU} algorithms to efﬁciently gather real-time, viewer-dependent global illumination onto both static and dynamic objects. These algorithms make different tradeoffs between performance and accuracy. Supplemental {GLSL} source code is included.},
	eventtitle = {I3D '17: Symposium on Interactive 3D Graphics and Games},
	pages = {1--11},
	booktitle = {Proceedings of the 21st {ACM} {SIGGRAPH} Symposium on Interactive 3D Graphics and Games},
	publisher = {{ACM}},
	author = {{McGuire}, Morgan and Mara, Mike and Nowrouzezahrai, Derek and Luebke, David},
	urldate = {2025-04-16},
	date = {2017-02-25},
	langid = {english},
	file = {PDF:C\:\\Users\\Jarod\\Zotero\\storage\\7XFXAWMA\\McGuire et al. - 2017 - Real-time global illumination using precomputed light field probes.pdf:application/pdf},
}

@online{noauthor_httpsresearchnvidiacomsitesdefaultfilespubs2017-02_real-time-global-illuminationlight-field-probes-finalpdf_nodate,
	title = {https://research.nvidia.com/sites/default/files/pubs/2017-02\_Real-Time-Global-Illumination/light-field-probes-final.pdf},
	url = {https://research.nvidia.com/sites/default/files/pubs/2017-02_Real-Time-Global-Illumination/light-field-probes-final.pdf},
	urldate = {2025-04-16},
	file = {https\://research.nvidia.com/sites/default/files/pubs/2017-02_Real-Time-Global-Illumination/light-field-probes-final.pdf:C\:\\Users\\Jarod\\Zotero\\storage\\4WHL6F3K\\light-field-probes-final.pdf:application/pdf},
}

@online{noauthor_httpsresearchnvidiacomsitesdefaultfilespubs2017-02_real-time-global-illuminationlight-field-probes-finalpdf_nodate-1,
	title = {https://research.nvidia.com/sites/default/files/pubs/2017-02\_Real-Time-Global-Illumination/light-field-probes-final.pdf},
	url = {https://research.nvidia.com/sites/default/files/pubs/2017-02_Real-Time-Global-Illumination/light-field-probes-final.pdf},
	urldate = {2025-04-16},
	file = {https\://research.nvidia.com/sites/default/files/pubs/2017-02_Real-Time-Global-Illumination/light-field-probes-final.pdf:C\:\\Users\\Jarod\\Zotero\\storage\\X577BWKI\\light-field-probes-final.pdf:application/pdf},
}

@online{noauthor_httpsresearchnvidiacomsitesdefaultfilespubs2017-02_real-time-global-illuminationlight-field-probes-finalpdf_nodate-2,
	title = {https://research.nvidia.com/sites/default/files/pubs/2017-02\_Real-Time-Global-Illumination/light-field-probes-final.pdf},
	url = {https://research.nvidia.com/sites/default/files/pubs/2017-02_Real-Time-Global-Illumination/light-field-probes-final.pdf},
	urldate = {2025-04-16},
	file = {https\://research.nvidia.com/sites/default/files/pubs/2017-02_Real-Time-Global-Illumination/light-field-probes-final.pdf:C\:\\Users\\Jarod\\Zotero\\storage\\HJITPHN8\\light-field-probes-final.pdf:application/pdf},
}

@online{noauthor_engine_2020-1,
	title = {Engine Work: Global Illumination with Irradiance Probes},
	url = {https://handmade.network/p/75/monter/blog/p/7288-engine_work__global_illumination_with_irradiance_probes},
	shorttitle = {Engine Work},
	abstract = {Global illumination is one key effect I want to achieve with Monter. For a low-poly style game, G…},
	titleaddon = {Handmade Network},
	urldate = {2025-04-19},
	date = {2020-04-21},
	langid = {american},
}

@article{majercik_dynamic_2019,
	title = {Dynamic Diffuse Global Illumination with Ray-Traced Irradiance Fields},
	volume = {8},
	abstract = {We show how to compute global illumination efﬁciently in scenes with dynamic objects and lighting. We extend classic irradiance probes to a compact encoding of the full irradiance ﬁeld in a scene. First, we compute the dynamic irradiance ﬁeld using an efﬁcient {GPU} memory layout, geometric ray tracing, and appropriate sampling rates without down-sampling or ﬁltering prohibitively large spherical textures. Second, we devise a robust ﬁltered irradiance query, using a novel visibility-aware moment-based interpolant. We experimentally validate performance and accuracy tradeoffs and show that our method of dynamic diffuse global illumination ({DDGI}) robustly lights scenes of varying geometric and radiometric complexity (Figure 1). For completeness, we demonstrate results with a state-of-the-art glossy ray-tracing term for sampling the full dynamic light ﬁeld and include reference {GLSL} code.},
	number = {2},
	author = {Majercik, Zander and Guertin, Jean-Philippe and Nowrouzezahrai, Derek and {McGuire}, Morgan},
	date = {2019},
	langid = {english},
	file = {PDF:C\:\\Users\\Jarod\\Zotero\\storage\\4HDBNLK3\\Majercik et al. - 2019 - Dynamic Diffuse Global Illumination with Ray-Traced Irradiance Fields.pdf:application/pdf},
}
